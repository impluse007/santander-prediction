{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np  # linear algebra\n",
    "import pandas as pd  # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import KFold\n",
    "from scipy.stats import skew\n",
    "import time\n",
    "import gc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.enable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.0\n"
     ]
    }
   ],
   "source": [
    "print(lgb.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data():\n",
    "    print('Reading data')\n",
    "    data = pd.read_csv('data/train.csv', nrows=None)\n",
    "    test = pd.read_csv('data/test.csv', nrows=None)\n",
    "    print('Train shape ', data.shape, ' Test shape ', test.shape)\n",
    "    return data, test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_selected_features():\n",
    "    return ['f190486d6', '58e2e02e6', 'eeb9cd3aa', '9fd594eec', '6eef030c1', '15ace8c9f', \n",
    "        'fb0f5dbfe', '58e056e12', '20aa07010', '024c577b9', 'd6bb78916', 'b43a7cfd5',\n",
    "        '58232a6fb', '1702b5bf0', '324921c7b', '62e59a501', '2ec5b290f', '241f0f867',\n",
    "        'fb49e4212', '66ace2992', 'f74e8f13d', '5c6487af1', '963a49cdc', '26fc93eb7',\n",
    "        '1931ccfdd', '703885424', '70feb1494', '491b9ee45', '23310aa6f', 'e176a204a',\n",
    "        '6619d81fc', '1db387535']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_statistics(train, test):\n",
    "    train_zeros = pd.DataFrame({'Percent_zero': ((train.values) == 0).mean(axis=0),\n",
    "                                'Column': train.columns})\n",
    "    \n",
    "    high_vol_columns = train_zeros['Column'][train_zeros['Percent_zero'] < 0.70].values\n",
    "    low_vol_columns = train_zeros['Column'][train_zeros['Percent_zero'] >= 0.70].values\n",
    "    train = train.replace({0:np.nan})\n",
    "    test = test.replace({0:np.nan})\n",
    "\n",
    "    cluster_sets = {\"low\":low_vol_columns, \"high\":high_vol_columns}\n",
    "    for cluster_key in cluster_sets:\n",
    "        for df in [train,test]:\n",
    "            df[\"count_not0_\"+cluster_key] = df[cluster_sets[cluster_key]].count(axis=1)\n",
    "            df[\"sum_\"+cluster_key] = df[cluster_sets[cluster_key]].sum(axis=1)\n",
    "            df[\"var_\"+cluster_key] = df[cluster_sets[cluster_key]].var(axis=1)\n",
    "            df[\"median_\"+cluster_key] = df[cluster_sets[cluster_key]].median(axis=1)\n",
    "            df[\"mean_\"+cluster_key] = df[cluster_sets[cluster_key]].mean(axis=1)\n",
    "            df[\"std_\"+cluster_key] = df[cluster_sets[cluster_key]].std(axis=1)\n",
    "            df[\"max_\"+cluster_key] = df[cluster_sets[cluster_key]].max(axis=1)\n",
    "            df[\"min_\"+cluster_key] = df[cluster_sets[cluster_key]].min(axis=1)\n",
    "            df[\"skew_\"+cluster_key] = df[cluster_sets[cluster_key]].skew(axis=1)\n",
    "            df[\"kurtosis_\"+cluster_key] = df[cluster_sets[cluster_key]].kurtosis(axis=1)\n",
    "    train_more_simplified = train.drop(high_vol_columns,axis=1).drop(low_vol_columns,axis=1)\n",
    "    colnames = list(train_more_simplified)\n",
    "    return train, test, colnames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_predict(data, y, test,colnames):\n",
    "    # Get the features we're going to train on\n",
    "    features = get_selected_features() + colnames #+ ['nb_nans', 'the_median', 'the_mean', 'the_sum', 'the_std', 'the_kur','the_max','the_min','the_var','count_not0']\n",
    "    # Create folds\n",
    "    folds = KFold(n_splits=8, shuffle=True, random_state=1)\n",
    "    # Convert to lightgbm Dataset\n",
    "    dtrain = lgb.Dataset(data=data[features], label=np.log1p(y['target']), free_raw_data=False)\n",
    "    # Construct dataset so that we can use slice()\n",
    "    dtrain.construct()\n",
    "    # Init predictions\n",
    "    sub_preds = np.zeros(test.shape[0])\n",
    "    oof_preds = np.zeros(data.shape[0])\n",
    "    # Lightgbm parameters\n",
    "    # Optimized version scores 0.40\n",
    "    # Step |   Time |      Score |      Stdev |   p1_leaf |   p2_subsamp |   p3_colsamp |   p4_gain |   p5_alph |   p6_lamb |   p7_weight |\n",
    "    #   41 | 00m04s |   -1.36098 |    0.02917 |    9.2508 |       0.7554 |       0.7995 |   -3.3108 |   -0.1635 |   -0.9460 |      0.6485 |\n",
    "    lgb_params = {\n",
    "        'objective': 'regression',\n",
    "        'num_leaves': 60,\n",
    "        'subsample': 0.6143,\n",
    "        'colsample_bytree': 0.6453,\n",
    "        'min_split_gain': np.power(10, -2.5988),\n",
    "        'reg_alpha': np.power(10, -2.2887),\n",
    "        'reg_lambda': np.power(10, 1.7570),\n",
    "        'min_child_weight': np.power(10, -0.1477),\n",
    "        'verbose': -1,\n",
    "        'seed': 11,\n",
    "        'boosting_type': 'gbdt',\n",
    "        'max_depth': -1,\n",
    "        'learning_rate': 0.03,\n",
    "        'metric': 'l1',\n",
    "    }\n",
    "    # Run KFold\n",
    "    for trn_idx, val_idx in folds.split(data):\n",
    "        # Train lightgbm\n",
    "        clf = lgb.train(\n",
    "            params=lgb_params,\n",
    "            train_set=dtrain.subset(trn_idx),\n",
    "            valid_sets=dtrain.subset(val_idx),\n",
    "            num_boost_round=10000,\n",
    "            early_stopping_rounds=100,\n",
    "            verbose_eval=50\n",
    "        )\n",
    "        # Predict Out Of Fold and Test targets\n",
    "        # Using lgb.train, predict will automatically select the best round for prediction\n",
    "        oof_preds[val_idx] = clf.predict(dtrain.data.iloc[val_idx])\n",
    "        sub_preds += clf.predict(test[features]) / folds.n_splits\n",
    "        # Display current fold score\n",
    "        print(mean_squared_error(np.log1p(y['target'].iloc[val_idx]),\n",
    "                                 oof_preds[val_idx]) ** .5)\n",
    "    # Display Full OOF score (square root of a sum is not the sum of square roots)\n",
    "    print('Full Out-Of-Fold score : %9.6f'\n",
    "          % (mean_squared_error(np.log1p(y['target']), oof_preds) ** .5))\n",
    "\n",
    "    return oof_preds, sub_preds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # Get the data\n",
    "    data, test = get_data()\n",
    "\n",
    "    # Get target and ids\n",
    "    y = data[['ID', 'target']].copy()\n",
    "    del data['target'], data['ID']\n",
    "    sub = test[['ID']].copy()\n",
    "    del test['ID']\n",
    "\n",
    "    # Free some memory\n",
    "    gc.collect()\n",
    "\n",
    "    # Add features\n",
    "    data, test, colnames = add_statistics(data, test)\n",
    "\n",
    "    # Predict test target\n",
    "    oof_preds, sub_preds = fit_predict(data, y, test, colnames)\n",
    "\n",
    "    # Store predictions\n",
    "    #y['predictions'] = np.expm1(oof_preds)\n",
    "    #y[['ID', 'target', 'predictions']].to_csv('reduced_set_oof.csv', index=False)\n",
    "    sub['target'] = np.expm1(sub_preds)\n",
    "    sub[['ID', 'target']].to_csv('leak.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading data\n",
      "Train shape  (4459, 4993)  Test shape  (49342, 4992)\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[50]\tvalid_0's l1: 1.11016\n",
      "[100]\tvalid_0's l1: 1.04463\n",
      "[150]\tvalid_0's l1: 1.03243\n",
      "[200]\tvalid_0's l1: 1.03307\n",
      "[250]\tvalid_0's l1: 1.03574\n",
      "Early stopping, best iteration is:\n",
      "[170]\tvalid_0's l1: 1.03088\n",
      "1.3186253236388124\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[50]\tvalid_0's l1: 1.13962\n",
      "[100]\tvalid_0's l1: 1.10367\n",
      "[150]\tvalid_0's l1: 1.09976\n",
      "[200]\tvalid_0's l1: 1.09651\n",
      "[250]\tvalid_0's l1: 1.10168\n",
      "[300]\tvalid_0's l1: 1.10297\n",
      "Early stopping, best iteration is:\n",
      "[202]\tvalid_0's l1: 1.09628\n",
      "1.4166634974687327\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[50]\tvalid_0's l1: 1.11337\n",
      "[100]\tvalid_0's l1: 1.05778\n",
      "[150]\tvalid_0's l1: 1.04528\n",
      "[200]\tvalid_0's l1: 1.04347\n",
      "[250]\tvalid_0's l1: 1.04567\n",
      "Early stopping, best iteration is:\n",
      "[183]\tvalid_0's l1: 1.04188\n",
      "1.3273981554583312\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[50]\tvalid_0's l1: 1.14568\n",
      "[100]\tvalid_0's l1: 1.07928\n",
      "[150]\tvalid_0's l1: 1.06196\n",
      "[200]\tvalid_0's l1: 1.05525\n",
      "[250]\tvalid_0's l1: 1.0558\n",
      "[300]\tvalid_0's l1: 1.05595\n",
      "Early stopping, best iteration is:\n",
      "[218]\tvalid_0's l1: 1.05359\n",
      "1.3454860810144837\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[50]\tvalid_0's l1: 1.14656\n",
      "[100]\tvalid_0's l1: 1.07295\n",
      "[150]\tvalid_0's l1: 1.05361\n",
      "[200]\tvalid_0's l1: 1.05067\n",
      "[250]\tvalid_0's l1: 1.05058\n",
      "[300]\tvalid_0's l1: 1.0546\n",
      "[350]\tvalid_0's l1: 1.05868\n",
      "Early stopping, best iteration is:\n",
      "[255]\tvalid_0's l1: 1.05007\n",
      "1.3636815622356409\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[50]\tvalid_0's l1: 1.10526\n",
      "[100]\tvalid_0's l1: 1.03881\n",
      "[150]\tvalid_0's l1: 1.03052\n",
      "[200]\tvalid_0's l1: 1.02721\n",
      "[250]\tvalid_0's l1: 1.02993\n",
      "Early stopping, best iteration is:\n",
      "[179]\tvalid_0's l1: 1.02687\n",
      "1.3437607669356153\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[50]\tvalid_0's l1: 1.1708\n",
      "[100]\tvalid_0's l1: 1.10364\n",
      "[150]\tvalid_0's l1: 1.08322\n",
      "[200]\tvalid_0's l1: 1.07638\n",
      "[250]\tvalid_0's l1: 1.07385\n",
      "[300]\tvalid_0's l1: 1.07438\n",
      "[350]\tvalid_0's l1: 1.07588\n",
      "Early stopping, best iteration is:\n",
      "[269]\tvalid_0's l1: 1.07272\n",
      "1.3715245623968892\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[50]\tvalid_0's l1: 1.05971\n",
      "[100]\tvalid_0's l1: 1.00788\n",
      "[150]\tvalid_0's l1: 0.995089\n",
      "[200]\tvalid_0's l1: 0.993299\n",
      "[250]\tvalid_0's l1: 0.994925\n",
      "[300]\tvalid_0's l1: 0.996143\n",
      "Early stopping, best iteration is:\n",
      "[213]\tvalid_0's l1: 0.992486\n",
      "1.2983841676941086\n",
      "Full Out-Of-Fold score :  1.348625\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
